variables:
  ### AWS
  AWS_REGION: us-east-1
  ### Application
  LOCAL_API_URL: http://localhost:5000/posts
  REVIEW_API_URL: https://review.devops-product-hunting.com:5000/posts
  PROD_API_URL: https://devops-product-hunting.com:5000/posts
  ### Environments
  REVIEW_URL: https://review.devops-product-hunting.com
  PROD_URL: https://devops-product-hunting.com
  ### FinOps
  TF_ROOT: infrastructure/prod
### Project CI/CD variables
# AWS_ACCESS_KEY_ID             - Protected
# AWS_ACCOUNT_ID                - Protected
# AWS_SECRET_ACCESS_KEY         - Protected/Masked
# GITLAB_TOKEN                  - Masked
# INFRACOST_API_KEY             - Masked
# MY_IP                         - Protected/Masked
# POSTGRES_DB                   - Protected
# POSTGRES_PASSWORD             - Protected/Masked
# POSTGRES_USER                 - Protected
# PRODUCT_HUNT_API_ACCESS_TOKEN - Protected/Masked
# SQLIZER_API_KEY               - Protected/Masked

stages:
  - prerequisites
  - finops
  - infrastructure
  - build
  - test
  - release
  - deploy
  - prod prerequisites
  - prod infrastructure
  - prod build
  - prod release
  - prod deploy
  - prod operate
  - prod monitor

### ###
### Dev - Merge request
### ###

infracost on prod infra:
  stage: finops
  image:
    name: infracost/infracost:ci-0.10
    entrypoint: [""]
  only:
    - merge_requests
  script:
    # Clone the base branch of the pull request into a temp directory.
    - git clone $CI_REPOSITORY_URL --branch=$CI_MERGE_REQUEST_TARGET_BRANCH_NAME --single-branch /tmp/base
    ### Generate an Infracost cost snapshot from the comparison branch, so that Infracost can compare the cost difference.
    - |
      infracost breakdown \
      --path=/tmp/base/${TF_ROOT} \
      --format=json \
      --out-file=infracost-base.json
    ### Generate an Infracost diff and save it to a JSON file.
    - |
      infracost diff \
      --path=${TF_ROOT} \
      --compare-to=infracost-base.json \
      --format=json \
      --out-file=infracost.json
    ### Post a comment to the PR using the 'update' behavior.
    - |
      infracost comment gitlab \
      --path=infracost.json \
      --repo=$CI_PROJECT_PATH \
      --merge-request=$CI_MERGE_REQUEST_IID \
      --gitlab-server-url=$CI_SERVER_URL \
      --gitlab-token=$GITLAB_TOKEN \
      --behavior=update

build client-server app:
  stage: build
  image: alpine:3.16
  only:
    - merge_requests
  cache:
    -
      key: DEV-NODE-MODULES
      paths:
        - $CI_PROJECT_DIR/application/backend/api/node_modules/
        - $CI_PROJECT_DIR/application/frontend/client/node_modules/
  before_script:
    - apk add --no-cache nodejs npm
  script:
    ### ### Server-side
    - cd $CI_PROJECT_DIR/application/backend/api
    ### Base dependencies
    - npm install express pg cors
    ### OpenTelemetry dependencies
    - npm install --save @opentelemetry/api @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node @opentelemetry/exporter-jaeger
    ### ### Client-side
    - cd $CI_PROJECT_DIR/application/frontend/client
    - npm install

smoke test:
  stage: test
  image: alpine:3.16
  services:
    - postgres:alpine3.16
  variables:
    POSTGRES_USER: test_user
    POSTGRES_PASSWORD: test_password
    POSTGRES_DB: test_db
  only:
    - merge_requests
  cache:
    -
      key: DEV-NODE-MODULES
      paths:
        - $CI_PROJECT_DIR/application/backend/api/node_modules/
        - $CI_PROJECT_DIR/application/frontend/client/node_modules/
  before_script:
    - apk add --no-cache postgresql-client nodejs npm
    ### Packages for Selenium
    - apk add --no-cache python3 py3-pip chromium-chromedriver gcc python3-dev libffi-dev musl-dev
    - pip3 install selenium
  script:
    ### Push .sql file to Postgres DB
    - cd $CI_PROJECT_DIR/merge-request
    - export PGPASSWORD="$POSTGRES_PASSWORD"
    - psql -h postgres -U $POSTGRES_USER -d $POSTGRES_DB -f sample_posts.sql
    ### Launch server API
    - cd $CI_PROJECT_DIR/application/backend/api
    - export POSTGRES_HOST="postgres"
    - export POSTGRES_PORT=5432
    - node --require './tracing.js' index.js &
    ### Launch client Web application
    - cd $CI_PROJECT_DIR/application/frontend/client
    - export REACT_APP_API_URL="$LOCAL_API_URL"
    - npm start &
    ### Wait for application to start
    - sleep 15
    ### Smoke test with Selenium
    - cd $CI_PROJECT_DIR/test/smoke-test
    - python3 extractWebPage.py
    - cat page.html | grep "Rank 500"
  artifacts:
    paths:
      - test/smoke-test/page.html

include:
  - template: Code-Quality.gitlab-ci.yml
code_quality:
  ### Execute this job only during merge requests
  rules:
    - if: $CODE_QUALITY_DISABLED
      when: never
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
  variables:
    REPORT_FORMAT: html
  artifacts:
    paths: [gl-code-quality-report.html]

### ###
### Review
### ###

verify state locking:
  stage: prerequisites
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    ### Verify if state-lock table exists (i.e. if state lock applied)
    - aws dynamodb describe-table --table-name product-hunting-terraform-state-lock
    ### Verify is S3 state storage exists
    - aws s3 ls product-hunting-terraform-state

apply state locking:
  stage: prerequisites
  needs: ["verify state locking"]
  when: on_failure
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./state-lock
    - terraform init
    - terraform apply -auto-approve

create main infra:
  stage: infrastructure
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./infrastructure/main
    - terraform init
    - terraform apply -auto-approve

store top posts:
  stage: build
  image: alpine:3.16
  only:
    - main
  cache:
    -
      key: REVIEW-POSTGRES
      paths:
        - $CI_PROJECT_DIR/application/backend/worker/posts.sql
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install sqlizer-io-client
  script:
    - cd ./application/backend/worker
    ### GET top 500 most voted posts from Product Hunt API (output .json file)
    - export API_ACCESS_TOKEN=$PRODUCT_HUNT_API_ACCESS_TOKEN
    - python3 getTopPosts.py
    ### Delete emojis inside .json file
    - sed -i -e 's/\(\\u\).\{4\}//g' posts.json
    ### Convert JSON to SQL
    - export API_KEY=$SQLIZER_API_KEY
    - python3 convertJsonToSql.py
    ### Reformat fields name & add id field
    - sed -i 's/list_node_//g' posts.sql
    - sed -i '2s/^/    "id" SERIAL PRIMARY KEY,\n/' posts.sql
    - sed -i "s/('/(DEFAULT,'/g" posts.sql

build project:
  stage: build
  image: alpine:3.16
  only:
    - main
  cache:
    -
      key: REVIEW-NODE-MODULES
      paths:
        - $CI_PROJECT_DIR/application/backend/api/node_modules/
    -
      key: REVIEW-BUILD
      paths:
        - $CI_PROJECT_DIR/application/frontend/client/build
  before_script:
    - apk add --no-cache npm
  script:
    ### ### Server-side
    - cd $CI_PROJECT_DIR/application/backend/api
    ### Base dependencies
    - npm install express pg cors
    ### OpenTelemetry dependencies
    - npm install --save @opentelemetry/api @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node @opentelemetry/exporter-jaeger
    ### ### Client-side
    - cd $CI_PROJECT_DIR/application/frontend/client
    - npm install
    - export REACT_APP_API_URL="$REVIEW_API_URL"
    - npm run build

create review docker images:
  stage: release
  image: docker:20.10
  services:
    - docker:20.10-dind
  only:
    - main
  cache:
    -
      key: REVIEW-POSTGRES
      paths:
        - $CI_PROJECT_DIR/application/backend/worker/posts.sql
    -
      key: REVIEW-NODE-MODULES
      paths:
        - $CI_PROJECT_DIR/application/backend/api/node_modules/
    -
      key: REVIEW-BUILD
      paths:
        - $CI_PROJECT_DIR/application/frontend/client/build/
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./release
    ### Build images
    - |
      docker-compose build \
      --build-arg POSTGRES_USER=$POSTGRES_USER \
      --build-arg POSTGRES_PASSWORD=$POSTGRES_PASSWORD \
      --build-arg POSTGRES_DB=$POSTGRES_DB \
      --no-cache
    ### Connect to AWS ECR registry
    - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
    ### Create image tags
    - docker tag product-hunting-postgres:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-postgres:review-$CI_COMMIT_SHORT_SHA
    - docker tag product-hunting-api:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-api:review-$CI_COMMIT_SHORT_SHA
    - docker tag product-hunting-client:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-client:review-$CI_COMMIT_SHORT_SHA
    ### Push images to ECR
    - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-postgres:review-$CI_COMMIT_SHORT_SHA
    - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-api:review-$CI_COMMIT_SHORT_SHA
    - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-client:review-$CI_COMMIT_SHORT_SHA

deploy review on ecs:
  stage: deploy
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  environment:
    name: review
    url: $REVIEW_URL
    on_stop: destroy review
  before_script:
    - apk add --no-cache python3 py3-pip
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
  script:
    ### Configure Terraform
    - cd ./deploy/ecs
    - |
      cat <<EOF | tee ./main.tfvars
      ecr_registry = "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"
      aws_account_id = "$AWS_ACCOUNT_ID"
      my_ip = "$MY_IP"
      ci_commit_short_sha = "$CI_COMMIT_SHORT_SHA"
      EOF
    - terraform init
    - terraform apply -var-file=main.tfvars -auto-approve

destroy review:
  stage: deploy
  needs: ["deploy review on ecs"]
  when: manual
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  environment:
    name: review
    action: stop
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./deploy/ecs
    - |
      cat <<EOF | tee ./main.tfvars
      ecr_registry = "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"
      aws_account_id = "$AWS_ACCOUNT_ID"
      my_ip = "$MY_IP"
      ci_commit_short_sha = "$CI_COMMIT_SHORT_SHA"
      EOF
    - terraform init
    - terraform destroy -var-file=main.tfvars -auto-approve

### ###
### Production
### ###

push to prod:
  stage: prod prerequisites
  needs: ["deploy review on ecs"]
  when: manual
  variables:
    GIT_STRATEGY: none
  only:
    - main
  script:
    - ""

create prod infra:
  stage: prod infrastructure
  needs: ["push to prod"]
  when: on_success
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./infrastructure/prod
    - terraform init
    - terraform apply -auto-approve

rebuild project for prod:
  stage: prod build
  needs: ["push to prod"]
  when: on_success
  image: alpine:3.16
  only:
    - main
  cache:
    -
      key: PROD-BUILD
      paths:
        - $CI_PROJECT_DIR/application/frontend/client/build
  before_script:
    - apk add --no-cache npm
  script:
    - cd $CI_PROJECT_DIR/application/frontend/client
    - npm install
    - export REACT_APP_API_URL="$PROD_API_URL"
    - npm run build

create prod docker images:
  stage: prod release
  needs: ["rebuild project for prod"]
  when: on_success
  image: docker:20.10
  services:
    - docker:20.10-dind
  only:
    - main
  cache:
    -
      key: PROD-BUILD
      paths:
        - $CI_PROJECT_DIR/application/frontend/client/build/
  before_script:
    - apk add --no-cache python3 py3-pip jq
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    ### Add tag to postgres image
    - MANIFEST=$(aws ecr batch-get-image --repository-name product-hunting-postgres --image-ids imageTag=review-$CI_COMMIT_SHORT_SHA --output json | jq --raw-output --join-output '.images[0].imageManifest')
    - aws ecr put-image --repository-name product-hunting-postgres --image-tag prod-$CI_COMMIT_SHORT_SHA --image-manifest "$MANIFEST"
    ### Add tag to api image
    - MANIFEST=$(aws ecr batch-get-image --repository-name product-hunting-api --image-ids imageTag=review-$CI_COMMIT_SHORT_SHA --output json | jq --raw-output --join-output '.images[0].imageManifest')
    - aws ecr put-image --repository-name product-hunting-api --image-tag prod-$CI_COMMIT_SHORT_SHA --image-manifest "$MANIFEST"
    ### Recreate client image
    - cd ./release
    - docker-compose build --no-cache client
    ### Connect to AWS ECR registry
    - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
    ### Create image tag
    - docker tag product-hunting-client:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-client:prod-$CI_COMMIT_SHORT_SHA
    ### Push image to ECR
    - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-client:prod-$CI_COMMIT_SHORT_SHA

deploy prod on eks:
  stage: prod deploy
  needs: ["create prod infra", "create prod docker images"]
  when: on_success
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  environment:
    name: production
    url: $PROD_URL
  before_script:
    - apk add --no-cache curl python3 py3-pip openssl
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Configure Terraform
    - cd ./deploy/eks
    - |
      cat <<EOF | tee ./main.tfvars
      ecr_registry = "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"
      ci_commit_short_sha = "$CI_COMMIT_SHORT_SHA"
      EOF
    - terraform init
    - terraform refresh -var-file=main.tfvars
    - terraform apply -var-file=main.tfvars -auto-approve
    ### Install Kubernetes Metrics Server
    - kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

apply vertical pod autoscaler:
  stage: prod operate
  needs: ["deploy prod on eks"]
  when: on_success
  image:
    name: golang:1.18-alpine3.16
    entrypoint: [""]
  only:
    - main
  before_script:
    - apk add --no-cache curl python3 py3-pip git bash openssl
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Install Vertical Pod Autoscaler (VPA)
    - cd ./operate/vpa
    - git clone https://github.com/kubernetes/autoscaler.git
    - bash ./autoscaler/vertical-pod-autoscaler/hack/vpa-up.sh
    ### Verify VPA installation
    - kubectl get pods -n kube-system | grep vpa
    ### Create VPA object
    - kubectl apply -f product-hunting-vpa.yaml

apply horizontal pod autoscaler:
  stage: prod operate
  needs: ["deploy prod on eks"]
  when: on_success
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  only:
    - main
  before_script:
    - apk add --no-cache curl python3 py3-pip
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Configure Terraform
    - cd ./operate/hpa
    - terraform init
    - terraform apply -auto-approve

rollback to previous revision:
  stage: prod operate
  needs: ["deploy prod on eks"]
  when: manual
  image: alpine:3.16
  only:
    - main
  before_script:
    - apk add --no-cache curl python3 py3-pip
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Perform a rollback to the previous revision
    - kubectl rollout undo deployment product-hunting

monitor vertical pod autoscaler:
  stage: prod monitor
  needs: ["apply vertical pod autoscaler"]
  when: manual
  image:
    name: golang:1.18-alpine3.16
    entrypoint: [""]
  only:
    - main
  before_script:
    - apk add --no-cache curl python3 py3-pip git bash
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Install Goldilocks
    - cd ./monitor/vpa
    - git clone https://github.com/FairwindsOps/goldilocks.git
    - cd goldilocks
    - kubectl create namespace goldilocks --dry-run=client -o yaml | kubectl apply -f -
    - kubectl -n goldilocks apply -f hack/manifests/controller
    - kubectl -n goldilocks apply -f hack/manifests/dashboard
    ### Enable 'default' namespace to Goldilocks Dashboard
    - kubectl label --overwrite ns default goldilocks.fairwinds.com/enabled=true
    ### ### Goldilocks is now accessible using port-forwarding
    ### kubectl -n goldilocks port-forward svc/goldilocks-dashboard 8080:80

install prometheus:
  stage: prod monitor
  needs: ["deploy prod on eks"]
  when: on_success
  image: alpine:3.16
  only:
    - main
  cache:
    -
      key: PROD-PROMETHEUS
      paths:
        - $CI_PROJECT_DIR/monitor/prometheus/manifests/
  before_script:
    - apk add --no-cache curl python3 py3-pip git
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Install kube-prometheus release-0.10 compatible with Kubernetes 1.22
    - cd ./monitor/prometheus
    - git clone --depth 1 https://github.com/prometheus-operator/kube-prometheus.git -b release-0.10 /tmp/prometheus
    - cp -R /tmp/prometheus/manifests .
    - kubectl apply --server-side -f manifests/setup

monitor kube with prometheus:
  stage: prod monitor
  needs: ["install prometheus"]
  when: manual
  image: alpine:3.16
  only:
    - main
  cache:
    -
      key: PROD-PROMETHEUS
      paths:
        - $CI_PROJECT_DIR/monitor/prometheus/manifests/
  before_script:
    - apk add --no-cache curl python3 py3-pip
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Deploy kube-prometheus
    - cd ./monitor/prometheus
    - kubectl apply -f manifests/
    ### ### Prometheus, Grafana and Alert Manager are now accessible using port-forwarding
    ### kubectl -n monitoring port-forward svc/prometheus-operated 9090
    ### kubectl -n monitoring port-forward svc/grafana 3000
    ### kubectl -n monitoring port-forward svc/alertmanager-main 9093

destroy prometheus:
  stage: prod monitor
  needs: ["monitor kube with prometheus"]
  when: manual
  image: alpine:3.16
  only:
    - main
  cache:
    -
      key: PROD-PROMETHEUS
      paths:
        - $CI_PROJECT_DIR/monitor/prometheus/manifests/
  before_script:
    - apk add --no-cache curl python3 py3-pip
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Destroy kube-prometheus
    - cd ./monitor/prometheus
    - kubectl delete --ignore-not-found=true -f manifests/

app tracing with jaeger:
  stage: prod monitor
  needs: ["deploy prod on eks"]
  when: manual
  image: alpine:3.16
  only:
    - main
  before_script:
    - apk add --no-cache curl python3 py3-pip
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Install cert-manager
    - kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.6.3/cert-manager.yaml
    ### Install Jaeger Operator
    - cd ./monitor/jaeger
    - kubectl apply -f jaeger-operator-role-binding.yml
    - kubectl create namespace observability
    - kubectl create -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.35.0/jaeger-operator.yaml -n observability
    ### Deploy Jaeger for Production
    - kubectl apply -f jaeger-prod.yml
    ### ### Jaeger is now accessible using port-forwarding
    ### kubectl port-forward svc/simple-prod-query 16686
