variables:
  AWS_REGION: us-east-1
  LOCAL_API_URL: http://localhost:5000/posts
  DOMAIN_API_URL: http://devops-product-hunting.com:5000/posts

stages:
  - infra prerequisites
  - create infra
  - build
  - test
  - release
  - deploy

verify state locking:
  stage: infra prerequisites
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  before_script:
    - apk add --no-cache curl python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    ### See if state-lock table exists (i.e. if state lock applied)
    - aws dynamodb describe-table --table-name product-hunting-terraform-state-lock

apply state locking:
  stage: infra prerequisites
  needs: ["verify state locking"]
  when: on_failure
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  before_script:
    - apk add --no-cache curl python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./create-infrastructure/state-lock
    - terraform init
    - terraform apply -auto-approve

create production infrastructure:
  stage: create infra
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  before_script:
    - apk add --no-cache curl python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./create-infrastructure
    ### Add personal public IP inside .tfvars
    - |
      cat <<EOF | tee ./main.tfvars
      my_ip = "$MY_IP/32"
      EOF
    - terraform init
    - terraform apply -var-file=main.tfvars -auto-approve

insert top products to database:
  stage: build
  needs: ["verify state locking"]
  when: on_success
  image: alpine:3.16
  before_script:
    - apk add --no-cache python3 py3-pip
    - pip3 install sqlizer-io-client
  script:
    - cd ./application/backend/worker
    ### GET top 500 most voted posts from Product Hunt API (output .json file)
    - export API_ACCESS_TOKEN=$PRODUCT_HUNT_API_ACCESS_TOKEN
    - python3 getTopPosts.py
    ### Delete emojis inside .json file
    - sed -i -e 's/\(\\u\).\{4\}//g' posts.json
    ### Convert JSON to SQL
    - export API_KEY=$SQLIZER_API_KEY
    - python3 convertJsonToSql.py
    ### Reformat fields name & add id field
    - sed -i 's/list_node_//g' posts.sql
    - sed -i '2s/^/    "id" SERIAL PRIMARY KEY,\n/' posts.sql
    - sed -i "s/('/(DEFAULT,'/g" posts.sql
  cache:
    key: $CI_COMMIT_REF_SLUG-POSTGRES
    paths:
      - $CI_PROJECT_DIR/application/backend/worker/

build client-server application modules:
  stage: build
  needs: ["verify state locking"]
  when: on_success
  image: alpine:3.16
  cache:
    -
      key: $CI_COMMIT_REF_SLUG-NODE-MODULES
      paths:
        - $CI_PROJECT_DIR/application/backend/api/node_modules/
        - $CI_PROJECT_DIR/application/frontend/client/node_modules/
    -
      key: $CI_COMMIT_REF_SLUG-BUILD
      paths:
        - $CI_PROJECT_DIR/application/frontend/client/build
  before_script:
    - apk add --no-cache nodejs npm
  script:
    ### Server-side
    - cd $CI_PROJECT_DIR/application/backend/api
    - npm install express pg cors
    ### Client-side
    - cd $CI_PROJECT_DIR/application/frontend/client
    - npm install
    - export REACT_APP_API_URL="$DOMAIN_API_URL"
    - npm run build

smoke test:
  stage: test
  needs: ["insert top products to database", "build client-server application modules"]
  when: on_success
  image: alpine:3.16
  services:
    - postgres:alpine3.16
  variables:
    POSTGRES_USER: $POSTGRES_USER
    POSTGRES_PASSWORD: $POSTGRES_PASSWORD
    POSTGRES_DB: $POSTGRES_DB
    POSTGRES_HOST_AUTH_METHOD: trust
  cache:
    -
      key: $CI_COMMIT_REF_SLUG-NODE-MODULES
      paths:
        - $CI_PROJECT_DIR/application/backend/api/node_modules/
        - $CI_PROJECT_DIR/application/frontend/client/node_modules/
    -
      key: $CI_COMMIT_REF_SLUG-POSTGRES
      paths:
        - $CI_PROJECT_DIR/application/backend/worker/
  before_script:
    - apk add --no-cache postgresql-client nodejs npm
    ### Packages for Selenium
    - apk add --no-cache python3 py3-pip chromium-chromedriver gcc python3-dev libffi-dev musl-dev
    - pip3 install selenium
  script:
    ### Push .sql file to Postgres DB
    - cd $CI_PROJECT_DIR/application/backend/worker
    - psql -h postgres -U $POSTGRES_USER -d $POSTGRES_DB -f posts.sql
    ### Launch server API
    - cd $CI_PROJECT_DIR/application/backend/api
    - export POSTGRES_HOST="postgres"
    - export POSTGRES_PORT=5432
    - node index index.js &
    ### Launch client Web application
    - cd $CI_PROJECT_DIR/application/frontend/client
    - export REACT_APP_API_URL="$LOCAL_API_URL"
    - npm start &
    ### Wait for application to start
    - sleep 15
    ### Smoke test with Selenium
    - cd $CI_PROJECT_DIR/test/smoke-test
    - python3 extractWebPage.py
    - cat page.html | grep "Rank 500"
  artifacts:
    paths:
      - test/smoke-test/page.html

include:
  - template: Code-Quality.gitlab-ci.yml
code_quality:
  variables:
    REPORT_FORMAT: html
  artifacts:
    paths: [gl-code-quality-report.html]

create docker images:
  stage: release
  image: docker:20.10
  services:
    - docker:20.10-dind
  cache:
    -
      key: $CI_COMMIT_REF_SLUG-NODE-MODULES
      paths:
        - $CI_PROJECT_DIR/application/backend/api/node_modules/
    -
      key: $CI_COMMIT_REF_SLUG-BUILD
      paths:
        - $CI_PROJECT_DIR/application/frontend/client/build/
    -
      key: $CI_COMMIT_REF_SLUG-POSTGRES
      paths:
        - $CI_PROJECT_DIR/application/backend/worker/
  before_script:
    - apk add --no-cache curl python3 py3-pip
    - pip3 install awscli
    - aws configure set region $AWS_REGION
  script:
    - cd ./docker
    ### Build images
    - |
      docker-compose build \
      --build-arg POSTGRES_USER=$POSTGRES_USER \
      --build-arg POSTGRES_PASSWORD=$POSTGRES_PASSWORD \
      --build-arg POSTGRES_DB=$POSTGRES_DB \
      --no-cache
    ### Connect to AWS ECR registry
    - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
    ### Create image tags
    - docker tag product-hunting-postgres:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-postgres:latest
    - docker tag product-hunting-api:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-api:latest
    - docker tag product-hunting-client:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-client:latest
    ### Push images to ECR
    - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-postgres:latest
    - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-api:latest
    - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/product-hunting-client:latest

deploy app with kube:
  stage: deploy
  image:
    name: hashicorp/terraform:1.2.2
    entrypoint: [""]
  before_script:
    - apk add --no-cache curl python3 py3-pip
    ### Install awscli
    - pip3 install awscli
    - aws configure set region $AWS_REGION
    ### Install Docker
    - apk add --no-cache docker openrc
    ### Install kubectl
    - curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
    - chmod +x ./kubectl
    - mv ./kubectl /usr/local/bin/kubectl
  script:
    ### Install EKS kubeconfig file locally
    - aws eks update-kubeconfig --name product-hunting-eks-cluster
    ### Connect to AWS ECR registry
    - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
    ### Configure Terraform
    - cd ./deploy-k8s
    - |
      cat <<EOF | tee ./main.tfvars
      ecr_registry = "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"
      EOF
    - terraform init
    - terraform refresh -var-file=main.tfvars
    - terraform apply -var-file=main.tfvars -auto-approve
    ### Add data to DB:
    - |
      LIST_PODS=$(kubectl get pods --no-headers -o custom-columns=":metadata.name" | grep product-hunting)
      for pod in $LIST_PODS; do
          IS_TABLE_CREATED=$(kubectl exec $pod --container product-hunting-postgres -- psql -h localhost -U $POSTGRES_USER -d $POSTGRES_DB -c '\dt' | { grep posts || true; })
          if [ -z "$IS_TABLE_CREATED" ]; then
              kubectl exec $pod --container product-hunting-postgres -- psql -h localhost -U $POSTGRES_USER -d $POSTGRES_DB -f posts.sql
          fi
      done
